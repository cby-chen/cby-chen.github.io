
<!DOCTYPE html>
<html lang="cn" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>搭建Hadoop2.7.2和Hive2.3.3以及Spark3.1.2 - 小陈运维</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="小陈运维,Hadoop 简介
Hadoop是一个用Java编写的Apache开源框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序在跨计算机集群提供分布式存储和计算的,"> 
    <meta name="author" content="chenby"> 
    <link rel="alternative" href="atom.xml" title="小陈运维" type="application/atom+xml"> 
    <link rel="icon" href="/img/1.ico"> 
    
    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="搭建Hadoop2.7.2和Hive2.3.3以及Spark3.1.2 - 小陈运维"/>
    <meta name="twitter:description" content="小陈运维,Hadoop 简介
Hadoop是一个用Java编写的Apache开源框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序在跨计算机集群提供分布式存储和计算的,"/>
    
    
    
    
    <meta property="og:site_name" content="小陈运维"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content="搭建Hadoop2.7.2和Hive2.3.3以及Spark3.1.2 - 小陈运维"/>
    <meta property="og:description" content="小陈运维,Hadoop 简介
Hadoop是一个用Java编写的Apache开源框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序在跨计算机集群提供分布式存储和计算的,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

<meta name="generator" content="Hexo 6.1.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">小陈运维</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">搭建Hadoop2.7.2和Hive2.3.3以及Spark3.1.2</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">搭建Hadoop2.7.2和Hive2.3.3以及Spark3.1.2</h1>
        <div class="stuff">
            <span>十二月 30, 2021</span>
            

        </div>
        <div class="content markdown">
            <p><strong>Hadoop 简介</strong></p>
<p>Hadoop是一个用Java编写的Apache开源框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。Hadoop框架工作的应用程序在跨计算机集群提供分布式存储和计算的环境中工作。Hadoop旨在从单个服务器扩展到数千个机器，每个都提供本地计算和存储。</p>
<p><strong>Hive简介</strong></p>
<p>Apache Hive是一个构建于Hadoop顶层的数据仓库，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。需要注意的是，Hive它并不是数据库。</p>
<p>Hive依赖于HDFS和MapReduce，其对HDFS的操作类似于SQL，我们称之为HQL，它提供了丰富的SQL查询方式来分析存储在HDFS中的数据。HQL可以编译转为MapReduce作业，完成查询、汇总、分析数据等工作。这样一来，即使不熟悉MapReduce 的用户也可以很方便地利用SQL 语言查询、汇总、分析数据。而MapReduce开发人员可以把己写的mapper 和reducer 作为插件来支持Hive 做更复杂的数据分析。</p>
<p><strong>Apache Spark 简介</strong></p>
<p>用于大数据工作负载的分布式开源处理系统</p>
<p>Apache Spark 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6afdfd92a4324cb1bfe1d8f726d59201~tplv-k3u1fbpfcp-zoom-1.image" alt="图片"></p>
<p><strong>本文将先搭建 jdk1.8 + MySQL5.7基础环境</strong></p>
<p><strong>之后搭建Hadoop2.7.2和Hive2.3.3以及Spark3.1.2</strong></p>
<p><strong>此文章搭建为单机版</strong>  </p>
<p><strong>1.创建目录并解压jdk安装包</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# mkdir  jdk<br>[root@localhost ~]# cd jdk&#x2F;<br>[root@localhost jdk]# ls<br>jdk-8u202-linux-x64.tar.gz<br>[root@localhost jdk]# ll<br>total 189496<br>-rw-r–r–. 1 root root 194042837 Oct 18 12:05 jdk-8u202-linux-x64.tar.gz<br>[root@localhost jdk]#<br>[root@localhost jdk]# tar xvf jdk-8u202-linux-x64.tar.gz</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>2.配置环境变量</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# vim &#x2F;etc&#x2F;profile<br>[root@localhost ~]# tail -n 3 &#x2F;etc&#x2F;profile<br>export JAVA_HOME&#x3D;&#x2F;root&#x2F;jdk&#x2F;jdk1.8.0_202&#x2F;<br>export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH<br>export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar<br>[root@localhost ~]#<br>[root@localhost ~]# source &#x2F;etc&#x2F;profile</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>3.下载安装MySQL并设置为开机自启</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# mkdir mysql<br>[root@localhost ~]# cd mysql<br>[root@localhost mysql]# wget <a target="_blank" rel="noopener" href="https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.35-1.el7.x86_64.rpm-bundle.tar">https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.35-1.el7.x86_64.rpm-bundle.tar</a><br>[root@localhost mysql]# tar xvf mysql-5.7.35-1.el7.x86_64.rpm-bundle.tar<br>[root@localhost mysql]# yum install .&#x2F;*.rpm<br>[root@localhost mysql]# systemctl start mysqld.service<br>[root@localhost mysql]#<br>[root@localhost mysql]# systemctl enable mysqld.service<br>[root@localhost mysql]#<br>[root@localhost mysql]#</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>4.查看MySQL默认密码，并修改默认密码，同时创建新的用户，将其设置为可以远程登录</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost mysql]# sudo grep ‘temporary password’ &#x2F;var&#x2F;log&#x2F;mysqld.log<br>2021-10-18T06:12:35.519726Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: eNHu&lt;sXHt3rq<br>[root@localhost mysql]#<br>[root@localhost mysql]#<br>[root@localhost mysql]# mysql -u root -p<br>Enter password:<br>Welcome to the MySQL monitor.  Commands end with ; or \g.<br>Your MySQL connection id is 9<br>Server version: 8.0.25</p>
<p>Copyright (c) 2000, 2021, Oracle and&#x2F;or its affiliates.</p>
<p>Oracle is a registered trademark of Oracle Corporation and&#x2F;or its<br>affiliates. Other names may be trademarks of their respective<br>owners.</p>
<p>Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement.</p>
<p>mysql&gt;<br>mysql&gt; ALTER USER ‘root‘@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘Cby123..’;<br>Query OK, 0 rows affected (0.02 sec)</p>
<p>mysql&gt;<br>mysql&gt;<br>mysql&gt; use mysql;<br>Database changed<br>mysql&gt;<br>mysql&gt; update user set host&#x3D;’%’ where user &#x3D;’root’;<br>Query OK, 1 row affected (0.01 sec)<br>Rows matched: 1  Changed: 1  Warnings: 0</p>
<p>mysql&gt; set global validate_password_policy&#x3D;0;<br>Query OK, 0 rows affected (0.01 sec)</p>
<p>mysql&gt; set global validate_password_mixed_case_count&#x3D;0;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt;  set global validate_password_number_count&#x3D;3;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; set global validate_password_special_char_count&#x3D;0;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; set global validate_password_length&#x3D;3;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; SHOW VARIABLES LIKE ‘validate_password%’;<br>+————————————–+——-+<br>| Variable_name                        | Value |<br>+————————————–+——-+<br>| validate_password_check_user_name    | OFF   |<br>| validate_password_dictionary_file    |       |<br>| validate_password_length             | 3     |<br>| validate_password_mixed_case_count   | 0     |<br>| validate_password_number_count       | 3     |<br>| validate_password_policy             | LOW   |<br>| validate_password_special_char_count | 0     |<br>+————————————–+——-+<br>7 rows in set (0.00 sec)</p>
<p>mysql&gt; create user ‘cby‘@’%’ identified by ‘cby’;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; grant all on <em>.</em> to ‘cby‘@’%’;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; FLUSH PRIVILEGES;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root‘@’%’WITH GRANT OPTION;<br>Query OK, 0 rows affected (0.00 sec)</p>
<p>mysql&gt; CREATE DATABASE dss_dev;<br>Query OK, 1 row affected (0.00 sec)</p>
<p>mysql&gt;<br>mysql&gt; select host,user,plugin from user;<br>+———–+—————+———————–+<br>| host      | user          | plugin                |<br>+———–+—————+———————–+<br>| %         | root          | mysql_native_password |<br>| localhost | mysql.session | mysql_native_password |<br>| localhost | mysql.sys     | mysql_native_password |<br>+———–+—————+———————–+<br>3 rows in set (0.01 sec)</p>
<p>mysql&gt;</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>注：若上面root不是mysql_native_password使用以下命令将其改掉</strong></p>
<p>&#96;&#96;&#96;shell<br>update user set plugin&#x3D;’mysql_native_password’ where user&#x3D;’root’;</p>
<p>&#96;&#96;&#96;shell</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/732afa3a5c5a4b23bc1fdfba94105597~tplv-k3u1fbpfcp-zoom-1.image" alt="图片"></p>
<p><strong>5.添加hosts解析，同时设置免密登录</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# mkdir Hadoop<br>[root@localhost ~]#<br>[root@localhost ~]# vim &#x2F;etc&#x2F;hosts<br>[root@localhost ~]#<br>[root@localhost ~]#<br>[root@localhost ~]# cat &#x2F;etc&#x2F;hosts<br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>127.0.0.1 namenode<br>[root@localhost ~]# ssh-keygen<br>[root@localhost ~]# ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub <a href="mailto:&#114;&#111;&#111;&#116;&#64;&#49;&#50;&#x37;&#x2e;&#x30;&#x2e;&#x30;&#x2e;&#49;">&#114;&#111;&#111;&#116;&#64;&#49;&#50;&#x37;&#x2e;&#x30;&#x2e;&#x30;&#x2e;&#49;</a><br>[root@localhost ~]#</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>6.下载Hadoop，解压后创建所需目录</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# cd Hadoop&#x2F;<br>[root@localhost Hadoop]# ls<br>[root@localhost Hadoop]# wget <a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/core/hadoop-2.7.2/hadoop-2.7.2.tar.gz">https://archive.apache.org/dist/hadoop/core/hadoop-2.7.2/hadoop-2.7.2.tar.gz</a><br>[root@localhost Hadoop]# tar xvf hadoop-2.7.2.tar.gz<br>[root@localhost Hadoop]#<br>[root@localhost Hadoop]# mkdir  -p &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;hadoopinfra&#x2F;hdfs&#x2F;namenode<br>[root@localhost Hadoop]#<br>[root@localhost Hadoop]#<br>[root@localhost Hadoop]# mkdir  -p &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;hadoopinfra&#x2F;hdfs&#x2F;datanode</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>7.添加Hadoop环境变量</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# vim &#x2F;etc&#x2F;profile<br>[root@localhost ~]# tail -n 8 &#x2F;etc&#x2F;profile<br>export HADOOP_HOME&#x3D;&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;<br>export HADOOP_INSTALL&#x3D;$HADOOP_HOME<br>export HADOOP_MAPRED_HOME&#x3D;$HADOOP_HOME<br>export HADOOP_COMMON_HOME&#x3D;$HADOOP_HOME<br>export HADOOP_HDFS_HOME&#x3D;$HADOOP_HOME<br>export YARN_HOME&#x3D;$HADOOP_HOME<br>export HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$HADOOP_HOME&#x2F;lib&#x2F;native<br>export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin:$HADOOP_HOME&#x2F;bin<br>[root@localhost ~]#<br>[root@localhost ~]# source &#x2F;etc&#x2F;profile</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>8.修改Hadoop配置</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# cd Hadoop&#x2F;hadoop-2.7.2&#x2F;<br>[root@localhost hadoop]# vim &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml<br>[root@localhost hadoop]#<br>[root@localhost hadoop]#<br>[root@localhost hadoop]# tail &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml</p>
<!-- Put site-specific property overrides in this file. -->


<configuration>
    <!-- 指定HDFS中NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://127.0.0.1:9000</value>
    </property>


<pre><code>&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/home/hadoop/hadoop-2.7.2/data/tmp&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</configuration>
```shell

  

<p><strong>9.修改Hadoop的hdfs目录配置</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost hadoop]# vim &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;hdfs-site.xml<br>[root@localhost hadoop]#<br>[root@localhost hadoop]#<br>[root@localhost hadoop]#<br>[root@localhost hadoop]# tail -n 15  &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;hdfs-site.xml<br><configuration><br>   <property><br>      <name>dfs.replication</name><br>      <value>1</value><br>   </property><br>   <property><br>      <name>dfs.name.dir</name><br>      <value>&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;hadoopinfra&#x2F;hdfs&#x2F;namenode</value><br>   </property><br>   <property><br>      <name>dfs.data.dir</name><br>      <value>&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;hadoopinfra&#x2F;hdfs&#x2F;datanode</value><br>   </property><br></configuration></p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>10.修改Hadoop的yarn配置</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost hadoop]#<br>[root@localhost hadoop]# vim  &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;yarn-site.xml<br>[root@localhost hadoop]#<br>[root@localhost hadoop]#<br>[root@localhost hadoop]# tail -n 6 &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;yarn-site.xml<br><configuration></p>
   <property> 
      <name>yarn.nodemanager.aux-services</name> 
      <value>mapreduce_shuffle</value> 
   </property>


</configuration>
```shell

  

<p>&#96;&#96;&#96;shell<br>[root@localhost hadoop]#<br>[root@localhost hadoop]# cp &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml.template &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml<br>[root@localhost hadoop]# vim &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml<br>[root@localhost hadoop]#<br>[root@localhost hadoop]#<br>[root@localhost hadoop]# tail &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml<br><configuration><br>   <property><br>      <name>mapreduce.framework.name</name><br>      <value>yarn</value><br>   </property><br></configuration><br>[root@localhost hadoop]#</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>11.修改Hadoop环境配置文件</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost hadoop]# vim &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh</p>
<p>修改JAVA_HOME<br>export JAVA_HOME&#x3D;&#x2F;root&#x2F;jdk&#x2F;jdk1.8.0_202&#x2F;</p>
<p>[root@localhost ~]# hdfs namenode -format<br>[root@localhost ~]# start-dfs.sh<br>[root@localhost ~]# start-yarn.sh</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>若重置太多次会导致clusterID不匹配，datanode起不来，删除版本后在初始化启动</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# rm -rf &#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;hadoopinfra&#x2F;hdfs&#x2F;datanode&#x2F;current&#x2F;VERSION<br>[root@localhost ~]# hadoop namenode -format<br>[root@localhost ~]# hdfs namenode -format<br>[root@localhost ~]# start-dfs.sh<br>[root@localhost ~]# start-yarn.sh</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>在浏览器访问Hadoop</strong></p>
<p>访问Hadoop的默认端口号为50070.使用以下网址，以获取浏览器Hadoop服务。</p>
<p><strong><a target="_blank" rel="noopener" href="http://localhost:50070/">http://localhost:50070/</a></strong></p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e6e5d35c40ba4ab1824e48f88a081536~tplv-k3u1fbpfcp-zoom-1.image" alt="图片"></p>
<p> <strong>验证集群的所有应用程序</strong></p>
<p>访问集群中的所有应用程序的默认端口号为8088。使用以下URL访问该服务。</p>
<p><strong><a target="_blank" rel="noopener" href="http://localhost:8088/">http://localhost:8088/</a></strong></p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a8ef9393ee3c422c8c1a3e1df487ef62~tplv-k3u1fbpfcp-zoom-1.image" alt="图片"></p>
<p><strong>12.创建hive目录并解压</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# mkdir hive<br>[root@localhost ~]# cd hive<br>[root@localhost hive]# wget <a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz">https://archive.apache.org/dist/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz</a><br>[root@localhost hive]# tar xvf apache-hive-2.3.3-bin.tar.gz<br>[root@localhost hive]#</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>13.备份hive配置文件</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost hive]# cd &#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;conf&#x2F;<br>[root@localhost conf]# cp hive-env.sh.template hive-env.sh<br>[root@localhost conf]# cp hive-default.xml.template hive-site.xml<br>[root@localhost conf]# cp hive-log4j2.properties.template hive-log4j2.properties<br>[root@localhost conf]# cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>14.在Hadoop中创建文件夹并设置权限</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost conf]# hadoop fs -mkdir -p &#x2F;data&#x2F;hive&#x2F;warehouse<br>21&#x2F;10&#x2F;18 14:27:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>[root@localhost conf]#<br>[root@localhost conf]# hadoop fs -mkdir &#x2F;data&#x2F;hive&#x2F;tmp<br>21&#x2F;10&#x2F;18 14:27:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>[root@localhost conf]#<br>[root@localhost conf]# hadoop fs -mkdir &#x2F;data&#x2F;hive&#x2F;log<br>21&#x2F;10&#x2F;18 14:27:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>[root@localhost conf]#<br>[root@localhost conf]# hadoop fs -chmod -R 777 &#x2F;data&#x2F;hive&#x2F;warehouse<br>21&#x2F;10&#x2F;18 14:27:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>[root@localhost conf]# hadoop fs -chmod -R 777 &#x2F;data&#x2F;hive&#x2F;tmp<br>21&#x2F;10&#x2F;18 14:27:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>[root@localhost conf]# hadoop fs -chmod -R 777 &#x2F;data&#x2F;hive&#x2F;log<br>21&#x2F;10&#x2F;18 14:27:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>[root@localhost conf]#</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>15.修改hive配置文件</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost conf]# vim hive-site.xml</p>
<p>hive 配置入下：<br><property><br>  <name>hive.exec.scratchdir</name><br>  <value>hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;data&#x2F;hive&#x2F;tmp</value><br></property><br><property><br>   <name>hive.metastore.warehouse.dir</name><br>  <value>hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;data&#x2F;hive&#x2F;warehouse</value><br></property><br><property><br>  <name>hive.querylog.location</name><br>  <value>hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;data&#x2F;hive&#x2F;log</value><br></property></p>
<p>&lt;!—该配置是关闭hive元数据版本认证，否则会在启动spark程序时报错–&gt;<br><property><br>  <name>hive.metastore.schema.verification</name><br>  <value>false</value><br></property></p>
<p>配置mysql IP 端口以及放元数据的库名称</p>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true</value>
</property>
<!—配置mysql启动器名称 -->
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
   <value>com.mysql.jdbc.Driver</value>
</property>
<!—配置连接mysql用户名 -->
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>root</value>
</property>
<!—配置连接mysql用户名登录密码-->
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>Cby123..</value>
</property>
```shell

  

  

  

<p><strong>修改配置文件中 system:java.io.tmpdir 和 system:user.name 相关信息，改为实际目录和用户名，或者加入如下配置</strong></p>
<p>&#96;&#96;&#96;shell<br>  <property><br>    <name>system:java.io.tmpdir</name><br>    <value>&#x2F;tmp&#x2F;hive&#x2F;java</value><br>  </property><br>  <property><br>    <name>system:user.name</name><br>    <value>${user.name}</value><br>  </property></p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>并修改临时路径 ：</strong></p>
<p>&#96;&#96;&#96;shell<br> <property><br>    <name>hive.exec.local.scratchdir</name><br>    <value>&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;tmp&#x2F;${system:user.name}</value><br>    <description>Local scratch space for Hive jobs</description><br>  </property><br>  <property><br>    <name>hive.downloaded.resources.dir</name><br>    <value>&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;tmp&#x2F;${hive.session.id}_resources</value><br>    <description>Temporary local directory for added resources in the remote file system.</description><br>  </property></p>
<property>
    <name>hive.server2.logging.operation.log.location</name>
    <value>/root/hive/apache-hive-2.3.3-bin/tmp/root/operation_logs</value>
    <description>Top level directory where operation logs are stored if logging functionality is enabled</description>
  </property>
```shell

  

<p><strong>16.配置hive中jdbc的MySQL驱动</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost lib]# cd &#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;lib&#x2F;<br>[root@localhost lib]# wget <a target="_blank" rel="noopener" href="https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.49.tar.gz">https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.49.tar.gz</a><br>[root@localhost lib]# tar xvf mysql-connector-java-5.1.49.tar.gz<br>[root@localhost lib]# cp mysql-connector-java-5.1.49&#x2F;mysql-connector-java-5.1.49.jar .<br>[root@localhost bin]#<br>[root@localhost bin]# vim &#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;conf&#x2F;hive-env.sh<br>[root@localhost bin]# tail -n 3 &#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;conf&#x2F;hive-env.sh<br>export HADOOP_HOME&#x3D;&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;<br>export HIVE_CONF_DIR&#x3D;&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;conf<br>export HIVE_AUX_JARS_PATH&#x3D;&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;lib<br>[root@localhost bin]#</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>17.配置hive环境变量</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# vim &#x2F;etc&#x2F;profile<br>[root@localhost ~]# tail -n 6 &#x2F;etc&#x2F;profile<br>export HADOOP_HOME&#x3D;&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;<br>export HIVE_CONF_DIR&#x3D;&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;conf<br>export HIVE_AUX_JARS_PATH&#x3D;&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;lib<br>export HIVE_PATH&#x3D;&#x2F;root&#x2F;hive&#x2F;apache-hive-2.3.3-bin&#x2F;<br>export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin:$HADOOP_HOME&#x2F;bin:$HIVE_PATH&#x2F;bin</p>
<p>[root@localhost bin]# .&#x2F;schematool -dbType mysql -initSchema</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>初始化完成后修改MySQL链接信息，之后配置mysql IP 端口以及放元数据的库名称</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost conf]# vim hive-site.xml</p>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://127.0.0.1:3306/hive?characterEncoding=utf8&amp;useSSL=false</value>
</property>

<p>[root@localhost bin]# nohup hive –service metastore &amp;<br>[root@localhost bin]# nohup hive –service hiveserver2 &amp;</p>
<p>&#96;&#96;&#96;shell</p>
<p>**18.创建spark目录并下载所需文件<br>**</p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# mkdir  spark<br>[root@localhost ~]#<br>[root@localhost ~]# cd spark<br>[root@localhost spark]#<br>[root@localhost spark]# wget <a target="_blank" rel="noopener" href="https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-without-hadoop.tgz">https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-without-hadoop.tgz</a> –no-check-certificate<br>[root@localhost spark]# tar xvf  spark-3.1.2-bin-without-hadoop.tgz</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>19.配置spark环境变量以及备份配置文件</strong></p>
<p>&#96;&#96;&#96;shell<br>[root@localhost ~]# vim &#x2F;etc&#x2F;profile<br>[root@localhost ~]#<br>[root@localhost ~]# tail -n 3 &#x2F;etc&#x2F;profile<br>export SPARK_HOME&#x3D;&#x2F;root&#x2F;spark&#x2F;spark-3.1.2-bin-without-hadoop&#x2F;<br>export PATH&#x3D;$PATH:$SPARK_HOME&#x2F;bin</p>
<p>[root@localhost spark]# cd &#x2F;root&#x2F;spark&#x2F;spark-3.1.2-bin-without-hadoop&#x2F;conf&#x2F;<br>[root@localhost conf]# cp spark-env.sh.template spark-env.sh<br>[root@localhost conf]# cp spark-defaults.conf.template spark-defaults.conf<br>[root@localhost conf]# cp metrics.properties.template metrics.properties</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>20.配置程序的环境变量</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost conf]# cp workers.template workers<br>[root@localhost conf]# vim spark-env.sh</p>
<p>export JAVA_HOME&#x3D;&#x2F;root&#x2F;jdk&#x2F;jdk1.8.0_202<br>export HADOOP_HOME&#x3D;&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2<br>export HADOOP_CONF_DIR&#x3D;&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;etc&#x2F;hadoop<br>export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;Hadoop&#x2F;hadoop-2.7.2&#x2F;bin&#x2F;hadoop classpath)<br>export SPARK_MASTER_HOST&#x3D;127.0.0.1<br>export SPARK_MASTER_PORT&#x3D;7077<br>export SPARK_HISTORY_OPTS&#x3D;”-Dspark.history.ui.port&#x3D;18080 -<br>Dspark.history.retainedApplications&#x3D;50 -<br>Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;spark-eventlog”</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>21.修改默认的配置文件</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost conf]# vim spark-defaults.conf</p>
<p>spark.master                     spark:&#x2F;&#x2F;127.0.0.1:7077<br>spark.eventLog.enabled           true<br>spark.eventLog.dir               hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;spark-eventlog<br>spark.serializer                 org.apache.spark.serializer.KryoSerializer<br>spark.driver.memory              3g<br>spark.eventLog.enabled           true<br>spark.eventLog.dir               hdfs:&#x2F;&#x2F;127.0.0.1:9000&#x2F;spark-eventlog<br>spark.eventLog.compress          true</p>
<p>&#96;&#96;&#96;shell</p>
<p><strong>22.配置工作节点</strong>  </p>
<p>&#96;&#96;&#96;shell<br>[root@localhost conf]# vim workers</p>
<p>[root@localhost conf]# cat workers<br>127.0.0.1</p>
<p>[root@localhost conf]# </p>
<p>[root@localhost sbin]# &#x2F;root&#x2F;spark&#x2F;spark-3.1.2-bin-without-hadoop&#x2F;sbin&#x2F;start-all.sh</p>
<p>&#96;&#96;&#96;shell</p>
<p> <strong>验证应用程序</strong></p>
<p>访问集群中的所有应用程序的默认端口号为8080。使用以下URL访问该服务。</p>
<p><strong><a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080/</a></strong></p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35679b2bcb784216a1ec18a153319e61~tplv-k3u1fbpfcp-zoom-1.image" alt="图片"></p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3fa0c9539aeb49eca25a60861911abd7~tplv-k3u1fbpfcp-zoom-1.image" alt="Linux运维交流社区"></p>
<p><strong>Linux运维交流社区</strong></p>
<p>Linux运维交流社区，互联网新闻以及技术交流。</p>
<p>41篇原创内容</p>
<p>公众号</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c010a1ccc0094b208340e06f26566146~tplv-k3u1fbpfcp-zoom-1.image" alt="图片"></p>
<blockquote>
<p>本文使用 <a target="_blank" rel="noopener" href="https://juejin.cn/post/6940875049587097631">文章同步助手</a> 同步</p>
</blockquote>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="true">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title="0" data-url="https://www.oiox.cn/1.mp3"></li>
                        
                    
                        
                            <li title="1" data-url="https://www.oiox.cn/2.mp3"></li>
                        
                    
                </ul>
            
        </div>
        
    <div id="gitalk-container" class="comment link"
		data-enable="false"
        data-ae="false"
        data-ci=""
        data-cs=""
        data-r=""
        data-o=""
        data-a=""
        data-d="false"
    >查看评论</div>


    </div>
    
        <div class="side">
				
        </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
